{"cells":[{"cell_type":"markdown","source":["Reference : https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b"],"metadata":{"id":"A7GWB7_kxqoP"}},{"cell_type":"markdown","metadata":{"id":"CpJKCdPDdSvx"},"source":["# Import Dependencies"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4426,"status":"ok","timestamp":1677212940777,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"vIMXNqM6c_iW","outputId":"534347b3-4dc0-4693-a5f4-7be2acdb5968"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras_preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras_preprocessing) (1.22.4)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from keras_preprocessing) (1.15.0)\n","Installing collected packages: keras_preprocessing\n","Successfully installed keras_preprocessing-1.1.2\n"]}],"source":["!pip install keras_preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1677212940778,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"DHSPioXoca14"},"outputs":[],"source":["import string\n","import numpy as np\n","\n","from keras.preprocessing.text import Tokenizer\n","# from keras.preprocessing.sequence import pad_sequences\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import LSTM,Input,TimeDistributed,Dense,Activation,RepeatVector,Embedding\n","from keras.optimizers import Adam\n","from keras.losses import sparse_categorical_crossentropy"]},{"cell_type":"markdown","metadata":{"id":"VCeFXCXScxNT"},"source":["# Loading data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":287,"status":"ok","timestamp":1677212949586,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"QM20DRpPc0FC"},"outputs":[],"source":["# Path to translation file\n","path_to_data = '/content/drive/MyDrive/AI_Research/spa.txt'"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677213868851,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"-3fqZuYZdXYI"},"outputs":[],"source":["# Read File\n","translation_file = open(path_to_data,\"r\", encoding='utf-8') \n","raw_data = translation_file.read()\n","translation_file.close()"]},{"cell_type":"markdown","metadata":{"id":"LHw2ykqEeQzF"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":1001,"status":"ok","timestamp":1677213870452,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"FJ5UpRePdrKw"},"outputs":[],"source":["# parse data\n","raw_data = raw_data.split('\\n')\n","pairs = [sentence.split('\\t') for sentence in raw_data]\n","pairs = pairs[1000:]"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322,"status":"ok","timestamp":1677213872040,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"Tk86pmCzgb9o","outputId":"c9135467-430c-454e-c786-493022dd51d9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Sit tight.',\n"," 'Tú mantente.',\n"," 'CC-BY 2.0 (France) Attribution: tatoeba.org #40196 (CM) & #5769260 (arh)']"]},"metadata":{},"execution_count":29}],"source":["pairs[10]"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1677213873524,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"ELUXChIWeJPz"},"outputs":[],"source":["# Remove Capital \n","\n","def clean_sentence(sentence):\n","  # Lower case the sentence\n","  lower_case_sent = sentence.lower()\n","\n","  # Strip punctuations\n","  string_punctuation = string.punctuation+'i'+ '¿'\n","  clean_sentence = lower_case_sent.translate(str.maketrans('','',string_punctuation))\n","\n","  return clean_sentence"]},{"cell_type":"markdown","metadata":{"id":"riNewJc8fQqp"},"source":["# Create Tokenizer"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677213873525,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"XgvtJbAFfL9i"},"outputs":[],"source":["def tokenize(sentences):\n","    # Create tokenizer\n","    text_tokenizer = Tokenizer()\n","    # Fit texts\n","    text_tokenizer.fit_on_texts(sentences)\n","    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"elapsed":330,"status":"error","timestamp":1677213873853,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":300},"id":"blQ1F11ffPv6","outputId":"8f4ad913-16b6-4041-db27-924661bbf395"},"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-8236be752915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Clean sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menglish_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspanish_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tokenize words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-8236be752915>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Clean sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menglish_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspanish_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tokenize words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["# Clean sentences\n","english_sentences = [clean_sentence(pair[0]) for pair in pairs]\n","spanish_sentences = [clean_sentence(pair[1]) for pair in pairs]\n","\n","# Tokenize words\n","spa_text_tokenized,spa_text_tokenizer = tokenize(spanish_sentences)\n","eng_text_tokenized,eng_text_tokenizer = tokenize(english_sentences)\n"]},{"cell_type":"markdown","source":["# Maximum Sentence Lengths"],"metadata":{"id":"tuVMJ_AljlRK"}},{"cell_type":"code","source":["# Finding Maximum Lengths of Sentences\n","print('Maximum length spanish sentence: {}'.format(len(max(spa_text_tokenized,key=len))))\n","print('Maximum length english sentence: {}'.format(len(max(eng_text_tokenized,key=len))))"],"metadata":{"id":"xGWeFqYChASJ","executionInfo":{"status":"aborted","timestamp":1677213873853,"user_tz":300,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vocabulary Size"],"metadata":{"id":"-c39SC6Fjn1f"}},{"cell_type":"code","source":["# Find Vocab Length\n","spanish_vocab = max(spa_text_tokenizer.word_index.values())\n","english_vocab = max(eng_text_tokenizer.word_index.values())\n","\n","print(\"Spanish vocabulary is of {} unique words\".format(spanish_vocab))\n","print(\"English vocabulary is of {} unique words\".format(english_vocab))"],"metadata":{"id":"jKedui9TiE5u","executionInfo":{"status":"aborted","timestamp":1677213873853,"user_tz":300,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Padding"],"metadata":{"id":"x47dAINcjiSj"}},{"cell_type":"code","source":["max_spanish_len = int(len(max(spa_text_tokenized,key=len)))\n","max_english_len = int(len(max(eng_text_tokenized,key=len)))"],"metadata":{"id":"guI_6aeAiPk_","executionInfo":{"status":"aborted","timestamp":1677213873853,"user_tz":300,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spa_pad_sentence = pad_sequences(spa_text_tokenized, max_spanish_len, padding = \"post\")\n","eng_pad_sentence = pad_sequences(eng_text_tokenized, max_english_len, padding = \"post\")"],"metadata":{"id":"wZS4LTPDjp9X","executionInfo":{"status":"aborted","timestamp":1677213873853,"user_tz":300,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reshape data\n","spa_pad_sentence = spa_pad_sentence.reshape(*spa_pad_sentence.shape, 1)\n","eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)"],"metadata":{"id":"mLawc61BjsVa","executionInfo":{"status":"aborted","timestamp":1677213873853,"user_tz":300,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Creation"],"metadata":{"id":"t7rh1faXkB9k"}},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"1QuKUN0qnqCe"}},{"cell_type":"code","source":["input_sequence = Input(shape=(max_spanish_len,))\n","embedding = Embedding(input_dim=spanish_vocab, output_dim=128,)(input_sequence)\n","encoder = LSTM(64, return_sequences=False)(embedding)\n","r_vec = RepeatVector(max_english_len)(encoder) # to have the hidden state vector to be passed in each time step of the decoder"],"metadata":{"id":"G_4TTTy_nq2E","executionInfo":{"status":"ok","timestamp":1677213873854,"user_tz":300,"elapsed":5,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"_CP7gjYUnr6U"}},{"cell_type":"code","source":["decoder = LSTM(64, return_sequences=True, dropout=0.2)(r_vec)\n","output = TimeDistributed(Dense(english_vocab,activation='relu'))(decoder) # TimeDistributed is used to apply the same Dense layer for each time step"],"metadata":{"id":"ktQHjfsZpZ8L","executionInfo":{"status":"ok","timestamp":1677213874890,"user_tz":300,"elapsed":1040,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["enc_dec_model =  Model(input_sequence, output)"],"metadata":{"id":"TSvUX8V4paKO","executionInfo":{"status":"ok","timestamp":1677213874890,"user_tz":300,"elapsed":6,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Compile the model\n","enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n","                     optimizer='adam',\n","                     metrics = ['accuracy'])\n","enc_dec_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XC9FA71xqmsk","executionInfo":{"status":"ok","timestamp":1677214435227,"user_tz":300,"elapsed":9,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"0ad51916-655d-4b12-f99b-ac3491b5d495"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 9)]               0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, 9, 128)            939776    \n","                                                                 \n"," lstm_2 (LSTM)               (None, 64)                49408     \n","                                                                 \n"," repeat_vector_1 (RepeatVect  (None, 5, 64)            0         \n"," or)                                                             \n","                                                                 \n"," lstm_3 (LSTM)               (None, 5, 64)             33024     \n","                                                                 \n"," time_distributed_1 (TimeDis  (None, 5, 3705)          240825    \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 1,263,033\n","Trainable params: 1,263,033\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Create early stopping callback\n","import tensorflow as tf\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.05,\n","                                                           patience=3, restore_best_weights=True)"],"metadata":{"id":"n-fXFEdjwMeM","executionInfo":{"status":"ok","timestamp":1677214438369,"user_tz":300,"elapsed":294,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Train the Model\n","\n","model_results = enc_dec_model.fit(spa_pad_sentence, eng_pad_sentence, batch_size=30, epochs=100,callbacks=early_stopping)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MgOC34NrPZ3","executionInfo":{"status":"ok","timestamp":1677214467626,"user_tz":300,"elapsed":28935,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"8bbc97a4-0677-4014-f2f9-0329e07d7334"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","634/634 [==============================] - 13s 14ms/step - loss: nan - accuracy: 0.3789\n","Epoch 2/100\n","634/634 [==============================] - 5s 9ms/step - loss: nan - accuracy: 0.3789\n","Epoch 3/100\n","634/634 [==============================] - 6s 9ms/step - loss: nan - accuracy: 0.3789\n","Epoch 4/100\n","634/634 [==============================] - 5s 8ms/step - loss: nan - accuracy: 0.3789\n"]}]},{"cell_type":"markdown","source":["# Making Translation"],"metadata":{"id":"wqI9u5hmrvtx"}},{"cell_type":"code","source":["def logits_to_sentence(logits, tokenizer):\n","\n","    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n","    index_to_words[0] = '<empty>' \n","\n","    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"],"metadata":{"id":"q7rcrq5Er4Dd","executionInfo":{"status":"ok","timestamp":1677214412279,"user_tz":300,"elapsed":371,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["index = 14\n","print(\"The english sentence is: {}\".format(english_sentences[index]))\n","print(\"The spanish sentence is: {}\".format(spanish_sentences[index]))\n","print('The predicted sentence is :')\n","print(logits_to_sentence(enc_dec_model.predict(spa_pad_sentence[index:index+1])[0], eng_text_tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFZjjXEtsFhh","executionInfo":{"status":"ok","timestamp":1677214413746,"user_tz":300,"elapsed":1097,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"045b89bf-e6fa-46f9-faa7-23865e7bfb27"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["The english sentence is: stay away\n","The spanish sentence is: aléjate\n","The predicted sentence is :\n","1/1 [==============================] - 1s 707ms/step\n","<empty> <empty> <empty> <empty> <empty>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"98NWX7pOsXCd","executionInfo":{"status":"aborted","timestamp":1677214133766,"user_tz":300,"elapsed":8,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1bepEcSFfXkAez_sdL0eUUyrbZeYXUXWb","authorship_tag":"ABX9TyPlD3xh5upjVcURYzOSuTmk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}